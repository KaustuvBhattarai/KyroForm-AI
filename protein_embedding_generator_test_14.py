# -*- coding: utf-8 -*-
"""Protein Embedding Generator Test 14

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kOv_cbkpXhjGaMxKYPZOGOBkWxsnGuVz
"""

pip install torch_geometric

pip install biopython

import torch
from transformers import AutoTokenizer, AutoModel
from Bio import SeqIO
import numpy as np
import os # For creating dummy file


def load_prottrans_model(model_name="Rostlab/prot_bert"): #Function to load ProtTrans model and tokenizer , returns a tuple containing the tokenizer and the model.
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if "half_fp16" in model_name:
            model = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16)
        else:
            model = AutoModel.from_pretrained(model_name)

        # Move model to GPU if available
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)
        model.eval() # Set model to evaluation mode
        print(f"Loaded model '{model_name}' on device: {device}")
        return tokenizer, model, device
    except Exception as e:
        print(f"Error is {e}")
        return None, None, None

def read_fasta_sequences(fasta_file_path):
    """
    Reads protein sequences from a FASTA file.
    """
    sequences = []
    try:
        for record in SeqIO.parse(fasta_file_path, "fasta"):
            # Replace non-standard amino acid characters (like 'U', 'O', 'B', 'Z', 'J', 'X') to not be treated as one
            # and convert to uppercase as ProtTrans models expect uppercase sequences
            clean_sequence = str(record.seq).replace("U", "X").replace("O", "X").replace("B", "X").replace("Z", "X").replace("J", "X").upper()
            sequences.append((record.id, clean_sequence))
        print(f"Successfully read {len(sequences)} sequences from {fasta_file_path}")
        return sequences
    except FileNotFoundError:
        print(f"Error: FASTA file not found at {fasta_file_path}")
        return []
    except Exception as e:
        print(f"Error reading FASTA file {fasta_file_path}: {e}")
        return []


def generate_protein_embeddings(sequences, tokenizer, model, device, batch_size=8): # returns a dictionary mapping sequence IDs to their ProtTrans embeddings numpy arrays.

    embeddings = {}
    total_sequences = len(sequences)
    print(f"Smbedding generation for {total_sequences} sequences")

    seq_ids = [s[0] for s in sequences]
    seq_strings = [s[1] for s in sequences] #preparing sequences for batching

    with torch.no_grad(): # Disable gradient calculation
        for i in range(0, total_sequences, batch_size):
            batch_ids = seq_ids[i:i+batch_size]
            batch_seqs = seq_strings[i:i+batch_size]

            # Join amino acids with spaces as required by ProtTrans tokenizers
            processed_batch_seqs = [" ".join(list(seq)) for seq in batch_seqs]

            # Tokenize the batch
            # `truncation=True` handles sequences longer than model's max ip length
            # `padding='longest'` pads sequences to the length of the longest in the batch
            inputs = tokenizer(processed_batch_seqs, return_tensors='pt', padding='longest', truncation=True)
            input_ids = inputs['input_ids'].to(device)
            attention_mask = inputs['attention_mask'].to(device)

            # Get model outputs (hidden states)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            #exclude where mask 0 importatnt

            # Get the last hidden state, shape (batch_size, sequence_length, embedding_dim)
            last_hidden_states = outputs.last_hidden_state

            for j, seq_id in enumerate(batch_ids):
                #The attention mask has 1 for real tokens and 0 for padding
                valid_length = attention_mask[j].sum()
                # Slice the hidden states to get only the valid tokens for this sequence
                # and then compute the mean
                # Exclude the first CLS and last EOS tokens if they exist and are not desired for pooling
                # i averaged all because lazy
                sequence_embedding = last_hidden_states[j, :valid_length].mean(dim=0)

                embeddings[seq_id] = sequence_embedding.cpu().numpy() # Move to CPU and convert to NumPy

            print(f"Processed batch {i // batch_size + 1}/{(total_sequences + batch_size - 1) // batch_size}")

    print("Embedding generation complete!")
    return embeddings

if __name__ == "__main__":
    # --- Create a dummy .fa file for demonstration ---
    dummy_fasta_content = """
>Seq1_human_protein
MNFQLFKRKLGLVHLDTKVAGKGLRMLFLGTSASRPRLGDARALFSRLR
>Seq2_bacterial_protein_HGT
MVTLPGVFVGPLKGDVVLEAGLELSRVGYEQNLLSGKRAILQLLVQLAEQ
>Seq3_another_human_protein
MESTPQVKKPNNKFLRRLLEQAKQELKKKLAEEQKKLVVEKSAALKKK
>Seq4_short_bacterial
MKKLM
"""
    fasta_file_path = "dummy_proteins.fa"
    with open(fasta_file_path, "w") as f:
        f.write(dummy_fasta_content.strip())
    print(f"Created dummy FASTA file: {fasta_file_path}")


    model_name_to_use = "Rostlab/prot_bert"
    tokenizer, model, device = load_prottrans_model(model_name_to_use)

    if tokenizer and model and device:
        protein_sequences_data = read_fasta_sequences(fasta_file_path)

        if protein_sequences_data:
            batch_size_for_embeddings = 8 # Use only 8 to 16 idk why

            protein_embeddings = generate_protein_embeddings(
                protein_sequences_data, tokenizer, model, device, batch_size=batch_size_for_embeddings
            )

            print("\nGenerated Embeddings first 3 entries")
            count = 0
            for seq_id, embedding in protein_embeddings.items():
                print(f"Sequence ID: {seq_id}")
                print(f"Embedding shape: {embedding.shape}") # ProtBERT typically produces 768-dim embeddings
                print(f"First 5 values: {embedding[:5]}\n")
                count += 1
                if count >= 3:
                    break

            output_embeddings_path = "protein_embeddings.npy"
            np.save(output_embeddings_path, protein_embeddings)
            print(f"Embeddings saved to: {output_embeddings_path}")

            # to load back use deez
            # loaded_embeddings = np.load(output_embeddings_path, allow_pickle=True).item()
            # print(f"\nLoaded embeddings: {list(loaded_embeddings.keys())[:2]}...")

    #cleaning dymmy file
    if os.path.exists(fasta_file_path):
        os.remove(fasta_file_path)
        print(f"\nCleaned up dummy FASTA file: {fasta_file_path}")

import numpy as np

output_embeddings_path = "protein_embeddings.npy"
loaded_embeddings = np.load(output_embeddings_path, allow_pickle=True).item()

print(f"loaded {len(loaded_embeddings)} protein embeddings")
# inspekt using
# for seq_id, embedding in list(loaded_embeddings.items())[:2]:
#     print(f"ID: {seq_id}, Embedding shape: {embedding.shape}")

from torch_geometric.data import HeteroData
import torch

data = HeteroData()

# Separate human and bacterial protein embeddings very important
human_protein_embeddings = []
human_protein_ids = []
bacterial_protein_embeddings = []
bacterial_protein_ids = []

for seq_id, embed in loaded_embeddings.items():
    if "human_protein" in seq_id: # A simple way to distinguish
        human_protein_embeddings.append(embed)
        human_protein_ids.append(seq_id)
    elif "bacterial_protein" in seq_id:
        bacterial_protein_embeddings.append(embed)
        bacterial_protein_ids.append(seq_id)

# array from numpy to tensor for torchu
data['human_protein'].x = torch.tensor(np.array(human_protein_embeddings), dtype=torch.float32)
data['bacterial_protein'].x = torch.tensor(np.array(bacterial_protein_embeddings), dtype=torch.float32)

# have to store mappings from original IDs or you cant graph node indices
data['human_protein'].node_ids = human_protein_ids
data['bacterial_protein'].node_ids = bacterial_protein_ids


print(data) #will add edge indices after

import numpy as np


output_embeddings_path = "protein_embeddings.npy"

try:

    # pickle is to be enabled cause its needed because the .npy file contains a dictionary
    loaded_embeddings = np.load(output_embeddings_path, allow_pickle=True).item()
    print(f"Successfully loaded {len(loaded_embeddings)} protein embeddings from {output_embeddings_path}")

    # to populate data['human_protein'].x and data['bacterial_protein'].x etc. reusable

except FileNotFoundError:
    print(f"file {output_embeddings_path} not foun")

except Exception as e:
    print(f"error  {e}")

"""```"""

import numpy as np
import matplotlib.pyplot as plt
import umap

# pip install umap-learn matplotlib

output_embeddings_path = "protein_embeddings.npy"

try:
    loaded_embeddings = np.load(output_embeddings_path, allow_pickle=True).item()
    print(f"loaded {len(loaded_embeddings)} protein embeddings from {output_embeddings_path}")
except FileNotFoundError:
    print(f" file {output_embeddings_path} was not found.")
    exit()
except Exception as e:
    print(f"error {e}")
    exit()

#dimensionality reduction needed
seq_ids = list(loaded_embeddings.keys())
embeddings_array = np.array(list(loaded_embeddings.values()))

print(f"Shape of combined embeddings array: {embeddings_array.shape}")
#classify for vis
protein_types = []
for seq_id in seq_ids:
    if "human_protein" in seq_id.lower():
        protein_types.append("Human")
    elif "bacterial_protein" in seq_id.lower() or "_hgt" in seq_id.lower():
        protein_types.append("Bacterial")
    else:
        protein_types.append("Other") # Catch any unclassified types

# Convert to a NumPy array for easier indexing
protein_types_array = np.array(protein_types)

#dimensionality reduction using umap used gpt for this idk how it works

reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)

reduced_embeddings = reducer.fit_transform(embeddings_array)
print(f"Shape of reduced embeddings: {reduced_embeddings.shape}")
#visualize finally

plt.figure(figsize=(10, 8))

# Get unique protein types for coloring
unique_types = np.unique(protein_types_array)
colors = plt.cm.get_cmap('viridis', len(unique_types)) # Use a colormap or define static colors?

for i, p_type in enumerate(unique_types):
    indices = np.where(protein_types_array == p_type)
    plt.scatter(
        reduced_embeddings[indices, 0],
        reduced_embeddings[indices, 1],
        color=colors(i),
        label=p_type,
        alpha=0.7,
        s=10 # points size
    )

plt.title('UMAP Projection of Protein Embeddings')
plt.xlabel('UMAP Component 1')
plt.ylabel('UMAP Component 2')
plt.legend(title='Protein Type')
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout() # Adjust layout to prevent labels from overlapping
plt.show()

print("\nVisualization complete! A plot window isbelow")

# Uncomment and rerun if you set n_components=3 in UMAP
# which iz basically for 3d visualization
#copied from somewhwere idk if it works but try : )

# from mpl_toolkits.mplot3d import Axes3D

# if reducer.n_components == 3:
#     fig = plt.figure(figsize=(12, 10))
#     ax = fig.add_subplot(111, projection='3d')

#     for i, p_type in enumerate(unique_types):
#         indices = np.where(protein_types_array == p_type)
#         ax.scatter(
#             reduced_embeddings[indices, 0],
#             reduced_embeddings[indices, 1],
#             reduced_embeddings[indices, 2],
#             color=colors(i),
#             label=p_type,
#             alpha=0.7,
#             s=10
#         )

#     ax.set_title('UMAP Projection of Protein Embeddings (3D)')
#     ax.set_xlabel('UMAP Component 1')
#     ax.set_ylabel('UMAP Component 2')
#     ax.set_zlabel('UMAP Component 3')
#     ax.legend(title='Protein Type')
#     plt.tight_layout()
#     plt.show()